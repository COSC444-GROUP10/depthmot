# Depth-Detection Fusion Module

This module combines depth estimation from MiDaS and object detection from YOLOv5 to create 3D descriptors for tracking.

## Overview

The fusion module takes the following approach:
1. Extract x, y coordinates from YOLOv5 bounding boxes
2. Sample depth (z) values from the MiDaS depth map within each bounding box
3. Create a combined 3D descriptor for each detected object

## Performance Optimization

To improve real-time performance, especially on hardware with limited resources:

- Depth estimation runs at a lower frame rate (every N frames, configurable)
- Object detection still runs on every frame
- The most recent depth map is reused for frames where depth estimation is skipped
- This significantly improves overall FPS while maintaining tracking quality

## Descriptor Format

Each 3D descriptor is a dictionary with the following keys:
- `bbox`: [x1, y1, x2, y2] (coordinates of the bounding box)
- `center_3d`: [x, y, z] (3D coordinates of the object center)
- `confidence`: Detection confidence
- `class_id`: Class ID
- `class_name`: Class name
- `depth_stats`: Dictionary with depth statistics within the box
  - `min`: Minimum depth value
  - `max`: Maximum depth value
  - `mean`: Mean depth value
  - `median`: Median depth value (used as the z-coordinate)
- `is_new_depth`: Boolean indicating if the depth was newly computed or reused

## Usage

```python
from depth.midas_depth import MiDaSDepthEstimator
from detection.yolo_detector import YOLOv5Detector
from fusion.descriptor_fusion import DepthDetectionFusion

# Initialize models
depth_estimator = MiDaSDepthEstimator()
object_detector = YOLOv5Detector()
fusion = DepthDetectionFusion(depth_estimator, object_detector, depth_processing_interval=3)

# Process a frame
frame = cv2.imread('image.jpg')
descriptors = fusion.process_frame(frame)

# Visualize results
vis_frame = fusion.visualize(frame, descriptors)
cv2.imshow('Visualization', vis_frame)
cv2.waitKey(0)
```

## Command Line Usage

```bash
# Run with default settings (depth every 3 frames)
python src/main.py --input 0

# Run with custom depth processing interval
python src/main.py --input video.mp4 --depth-interval 5

# Save output video
python src/main.py --input video.mp4 --output result.mp4 --depth-interval 3
```

## Integration with Tracking

The 3D descriptors generated by this module can be passed to a tracking algorithm. The tracking algorithm can use the 3D coordinates (x, y, z) for more robust tracking, especially in scenarios with occlusions or depth changes.

For example, a tracking algorithm could:
1. Use the 3D coordinates for motion prediction
2. Calculate 3D distances between detections and tracks
3. Handle occlusions based on depth information
4. Improve identity preservation by considering depth changes 